---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
# https://raw.githubusercontent.com/VictoriaMetrics/helm-charts/refs/heads/master/charts/victoria-metrics-k8s-stack/values.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: victoria-metrics
spec:
  chartRef:
    kind: OCIRepository
    name: victoria-metrics-k8s-stack
  interval: 1h
  postRenderers:
    - kustomize:
        patches:
          - target:
              kind: VMRule
              name: stack-k8s.rules.containermemoryswap
            patch: |-
              - op: replace
                path: /spec/groups/0/rules/0/expr
                value: 'container_memory_swap{job="kubelet",metrics_path="/metrics/cadvisor",image!=""} * on(cluster,namespace,pod) group_left(node) topk(1,max(kube_pod_info{node!=""}) by(cluster,namespace,pod,node)) by(cluster,namespace,pod) or vector(0)'
  values:
    fullnameOverride: stack
    victoria-metrics-operator:
      env:
        - name: VM_VMALERTDEFAULT_CONFIGRELOADERCPU
          value: 0
        - name: VM_VMAGENTDEFAULT_CONFIGRELOADERCPU
          value: 0
        - name: VM_VMALERTMANAGER_CONFIGRELOADERCPU
          value: 0
      operator:
        enable_converter_ownership: true # Required to allow VM to remove VM rules it imports if a prometheus rule is deleted
        useCustomConfigReloader: true # Bundled config-reloader, should reduce vmagent and vmauth config sync-time and make it predictable.

    defaultDashboards:
      enabled: false

    defaultRules:
      create: true
      # Added to not consider 304 redirects as errors/warnings
      groups:
        kubeApiserverSlos:
          create: false

    vmsingle:
      spec:
        # -- Data retention period. Possible units character: h(ours), d(ays), w(eeks), y(ears), if no unit character specified - month. The minimum retention period is 24h. See these [docs](https://docs.victoriametrics.com/single-server-victoriametrics/#retention)
        retentionPeriod: "6w"
        replicaCount: 1
        storage:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 40Gi
          storageClassName: ceph-ssd
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 3000m
            memory: 6Gi
      ingress:
        enabled: true
        ingressClassName: internal
        annotations:
          nginx.ingress.kubernetes.io/auth-url: |-
            http://ak-outpost-authentik-embedded-outpost.default.svc.cluster.local:9000/outpost.goauthentik.io/auth/nginx
          nginx.ingress.kubernetes.io/auth-signin: |-
            https://${APP}.${SECRET_DOMAIN}/outpost.goauthentik.io/start?rd=$scheme://$http_host$escaped_request_uri
          nginx.ingress.kubernetes.io/auth-response-headers: |-
            Set-Cookie,X-authentik-username,X-authentik-groups,X-authentik-email,X-authentik-name,X-authentik-uid,X-authentik-entitlements
          nginx.ingress.kubernetes.io/auth-snippet: |
            proxy_set_header X-Forwarded-Host $http_host;
        hosts:
          - "${APP}.${SECRET_DOMAIN}"

    alertmanager:
      useManagedConfig: true
      config:
        route:
          group_by: ["alertname", "job"]
          group_interval: 10m
          group_wait: 1m
          receiver: pushover
          repeat_interval: 12h
          routes:
            - receiver: "null"
              matchers:
                - alertname=InfoInhibitor
            - receiver: heartbeat
              group_interval: 15s
              group_wait: 0s
              repeat_interval: 5m
              matchers:
                - alertname=Watchdog
            - receiver: pushover
              matchers:
                - severity=~"warning|critical"
        inhibit_rules:
          - source_matchers:
              - severity = "critical"
            target_matchers:
              - severity = "warning"
            equal: ["alertname", "namespace"]
        receivers:
          - name: "null"
          - name: heartbeat
            webhook_configs:
              - url_secret:
                  name: &secret alertmanager-secret
                  key: ALERTMANAGER_HEARTBEAT_URL
          - name: pushover
            pushover_configs:
              - html: true
                message: |-
                  {{- range .Alerts }}
                    {{- if ne .Annotations.description "" }}
                      {{ .Annotations.description }}
                    {{- else if ne .Annotations.summary "" }}
                      {{ .Annotations.summary }}
                    {{- else if ne .Annotations.message "" }}
                      {{ .Annotations.message }}
                    {{- else }}
                      Alert description not available
                    {{- end }}
                    {{- if gt (len .Labels.SortedPairs) 0 }}
                      <small>
                        {{- range .Labels.SortedPairs }}
                          <b>{{ .Name }}:</b> {{ .Value }}
                        {{- end }}
                      </small>
                    {{- end }}
                  {{- end }}
                priority: |-
                  {{ if eq .Status "firing" }}1{{ else }}0{{ end }}
                send_resolved: true
                sound: gamelan
                title: >-
                  [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}]
                  {{ .CommonLabels.alertname }}
                ttl: 86400s
                token:
                  name: *secret
                  key: PUSHOVER_ALERTMANAGER_TOKEN
                user_key:
                  name: *secret
                  key: PUSHOVER_USER_KEY
                url_title: View in Alertmanager

    vmalert:
      spec:
        # Skip vmalerts for vmlog rules
        selectAllByDefault: false
        ruleSelector:
          matchExpressions:
            - key: vmalert-logs.io/enabled
              operator: NotIn
              values: ["true"]
        ruleNamespaceSelector:
          matchExpressions:
            - key: somekey
              operator: NotIn
              values: ["never-used-value"]

    vmagent:
      spec:
        extraArgs:
          promscrape.maxScrapeSize: 50MiB
          promscrape.streamParse: "true"
          promscrape.dropOriginalLabels: "true"
        resources:
          requests:
            cpu: 500m
            memory: 256Mi
          limits:
            memory: 512Mi

    grafana:
      enabled: false

    external:
      grafana:
        host: "https://grafana.${SECRET_DOMAIN}"
        datasource: Prometheus

    kube-state-metrics:
      livenessProbe:
        httpGet:
          path: /healthz
          port: telemetry
        initialDelaySeconds: 60
        periodSeconds:       20
        failureThreshold:    6
      readinessProbe:
        httpGet:
          path: /readyz
          port: telemetry
        initialDelaySeconds: 30
        periodSeconds:       15
        failureThreshold:    5

    prometheus-node-exporter:
      vmScrape:
        spec:
          endpoints:
            - port: metrics
              relabelConfigs:
                - source_labels:
                    - __meta_kubernetes_endpoint_node_name
                  target_label: node

    kubelet:
      vmScrape:
        spec:
          # drop high cardinality label and useless metrics for cadvisor and kubelet
          metricRelabelConfigs:
            # Drop less useful container CPU metrics.
            - sourceLabels: [__name__]
              action: drop
              regex: "container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)"
            # Drop less useful / always zero container memory metrics.
            - sourceLabels: [__name__]
              action: drop
              regex: "container_memory_(failures_total|mapped_file|swap)"
            # Drop less useful container process metrics.
            - sourceLabels: [__name__]
              action: drop
              # regex: 'container_(file_descriptors|tasks_state|threads_max)'
              regex: "container_(tasks_state|threads_max)"
            # Drop less useful container filesystem metrics.
            - sourceLabels: [__name__]
              action: drop
              regex: "container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)"
            # Drop less useful container blkio metrics.
            - sourceLabels: [__name__]
              action: drop
              regex: "container_blkio_device_usage_total"
            # Drop container spec metrics that overlap with kube-state-metrics.
            - sourceLabels: [__name__]
              action: drop
              regex: "container_spec.*"
            # Drop cgroup metrics with no pod.
            - sourceLabels: [id, pod]
              action: drop
              regex: ".+;"
            - action: drop
              sourceLabels: [__name__]
              regex: prober_probe_duration_seconds_bucket
            # Drop high-cardinality labels.
            - action: labeldrop
              regex: (uid|id|pod_uid|interface)
            - action: drop
              sourceLabels: [__name__]
              regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)

    kubeControllerManager:
      enabled: false

    kubeEtcd:
      enabled: false

    kubeScheduler:
      enabled: false

    kubeProxy:
      enabled: false

    additionalVictoriaMetricsMap:
      dockerhub-rules:
        create: true
        groups:
          - name: dockerhub
            rules:
              - alert: DockerhubRateLimitRisk
                annotations:
                  summary: Kubernetes cluster Dockerhub rate limit risk
                expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
                labels:
                  severity: critical
      oom-rules:
        create: true
        groups:
          - name: oom
            rules:
              - alert: OomKilled
                annotations:
                  summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                labels:
                  severity: critical

    extraObjects:
      - apiVersion: monitoring.coreos.com/v1
        kind: PrometheusRule
        metadata:
          name: custom-kube-apiserver-slos
          namespace: observability
        spec:
          groups:
            - name: kube-apiserver-slos
              rules:
              - alert: KubeAPIErrorBudgetBurn
                annotations:
                  description: 'The API server is burning too much error budget on cluster .'
                  runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn'
                  summary: 'The API server is burning too much error budget.'
                condition: 'true'
                expr: |-
                  sum by (cluster) (apiserver_request:burnrate1h{code=~"4..|5.."}) > (14.40 * 0.01000)
                  and on (cluster)
                  sum by (cluster) (apiserver_request:burnrate5m{code=~"4..|5.."}) > (14.40 * 0.01000)
                for: 2m
                labels:
                  long: 1h
                  severity: critical
                  short: 5m
              - alert: KubeAPIErrorBudgetBurn
                annotations:
                  description: 'The API server is burning too much error budget on cluster .'
                  runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn'
                  summary: 'The API server is burning too much error budget.'
                condition: 'true'
                expr: |-
                  sum by (cluster) (apiserver_request:burnrate6h{code=~"4..|5.."}) > (6.00 * 0.01000)
                  and on (cluster)
                  sum by (cluster) (apiserver_request:burnrate30m{code=~"4..|5.."}) > (6.00 * 0.01000)
                for: 15m
                labels:
                  long: 6h
                  severity: critical
                  short: 30m
              - alert: KubeAPIErrorBudgetBurn
                annotations:
                  description: 'The API server is burning too much error budget on cluster .'
                  runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn'
                  summary: 'The API server is burning too much error budget.'
                condition: 'true'
                expr: |-
                  sum by (cluster) (apiserver_request:burnrate1d{code=~"4..|5.."}) > (3.00 * 0.01000)
                  and on (cluster)
                  sum by (cluster) (apiserver_request:burnrate2h{code=~"4..|5.."}) > (3.00 * 0.01000)
                for: 1h
                labels:
                  long: 1d
                  severity: warning
                  short: 2h
              - alert: KubeAPIErrorBudgetBurn
                annotations:
                  description: 'The API server is burning too much error budget on cluster .'
                  runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn'
                  summary: 'The API server is burning too much error budget.'
                condition: 'true'
                expr: |-
                  sum by (cluster) (apiserver_request:burnrate3d{code=~"4..|5.."}) > (1.00 * 0.01000)
                  and on (cluster)
                  sum by (cluster) (apiserver_request:burnrate6h{code=~"4..|5.."}) > (1.00 * 0.01000)
                for: 3h
                labels:
                  long: 3d
                  severity: warning
                  short: 6h